{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/ml/apartin/anaconda3/envs/p1/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from glob import glob\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/ml/apartin/projects/DrugResponseViz/notebooks\n"
     ]
    }
   ],
   "source": [
    "runs_dir_name = 'out_lgbm'\n",
    "file_path = Path.cwd()\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_dir_path = Path(file_path / '..' / runs_dir_name)\n",
    "runs_dirs = [Path(p) for p in glob(str(runs_dir_path/'run_*'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(y_true, y_pred, mltype, metrics=None):\n",
    "    \"\"\" Create dict of scores.\n",
    "    Args:\n",
    "        metrics : TODO allow to pass a string of metrics\n",
    "    \"\"\"\n",
    "    import sklearn.metrics as metrics\n",
    "    scores = {}\n",
    "\n",
    "    if mltype == 'cls':    \n",
    "        scores['auroc'] = sklearn.metrics.roc_auc_score(y_true, y_pred)\n",
    "        scores['f1_score'] = sklearn.metrics.f1_score(y_true, y_pred, average='micro')\n",
    "        scores['acc_blnc'] = sklearn.metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    elif mltype == 'reg':\n",
    "#         scores['r2'] = sklearn.metrics.r2_score(y_true=y_true, y_pred=y_pred)\n",
    "#         scores['mean_absolute_error'] = sklearn.metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "#         scores['median_absolute_error'] = sklearn.metrics.median_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "#         # scores['mean_squared_error'] = sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "#         scores['mse'] = sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "#         scores['rmse'] = sqrt( sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred) )\n",
    "#         # scores['auroc_reg'] = reg_auroc(y_true=y_true, y_pred=y_pred)\n",
    "        \n",
    "        scores['r2'] = metrics.r2_score(y_true=y_true, y_pred=y_pred)\n",
    "        scores['mean_absolute_error'] = metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "        scores['median_absolute_error'] = metrics.median_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "        # scores['mean_squared_error'] = sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "        scores['mse'] = metrics.mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "        scores['rmse'] = sqrt( metrics.mean_squared_error(y_true=y_true, y_pred=y_pred) )        \n",
    "        \n",
    "    scores['y_avg_true'] = np.mean(y_true)\n",
    "    scores['y_avg_pred'] = np.mean(y_pred)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc(prd, mltype='reg'):\n",
    "    y_pred, y_true = prd['y_pred'], prd['y_true']\n",
    "    scores = calc_scores(y_true=y_true, y_pred=y_pred, mltype=mltype, metrics=None)\n",
    "    scores['run'] = str(d).split(os.sep)[-1]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /vol/ml/apartin/projects/DrugResponseViz/notebooks/../out_lgbm/run_s000\n",
      "Processing /vol/ml/apartin/projects/DrugResponseViz/notebooks/../out_lgbm/run_s020\n",
      "Processing /vol/ml/apartin/projects/DrugResponseViz/notebooks/../out_lgbm/run_s040\n",
      "Processing /vol/ml/apartin/projects/DrugResponseViz/notebooks/../out_lgbm/run_s060\n",
      "Processing /vol/ml/apartin/projects/DrugResponseViz/notebooks/../out_lgbm/run_s080\n"
     ]
    }
   ],
   "source": [
    "# Append scores (dicts)\n",
    "tr_scores_all = []\n",
    "vl_scores_all = []\n",
    "te_scores_all = []\n",
    "\n",
    "for i, d in enumerate(runs_dirs):\n",
    "    prd_tr = pd.read_csv(d/'preds_tr.csv')\n",
    "    prd_vl = pd.read_csv(d/'preds_vl.csv')\n",
    "    prd_te = pd.read_csv(d/'preds_te.csv')\n",
    "    # prd['err'] = abs(prd['y_true'] - prd['y_pred'])\n",
    "    \n",
    "    mltype = 'reg'\n",
    "    \n",
    "    tr_scores = fnc(prd=prd_tr)\n",
    "    vl_scores = fnc(prd=prd_vl)\n",
    "    te_scores = fnc(prd=prd_te)\n",
    "        \n",
    "    # Append scores (dicts)\n",
    "    tr_scores_all.append(tr_scores)\n",
    "    vl_scores_all.append(vl_scores)\n",
    "    te_scores_all.append(te_scores)\n",
    "    \n",
    "    if i%20==0:\n",
    "        print(f'Processing {d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_df(scores_all):\n",
    "    scores = pd.DataFrame(scores_all)\n",
    "    cols = scores.columns.tolist()\n",
    "    cols.remove('run')\n",
    "    scores = scores[['run'] + cols]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_scores_df = scores_to_df(tr_scores_all)\n",
    "vl_scores_df = scores_to_df(vl_scores_all)\n",
    "te_scores_df = scores_to_df(te_scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>median_absolute_error</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "      <th>y_avg_pred</th>\n",
       "      <th>y_avg_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run_s000</td>\n",
       "      <td>0.059128</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.007048</td>\n",
       "      <td>0.699714</td>\n",
       "      <td>0.083951</td>\n",
       "      <td>0.838018</td>\n",
       "      <td>0.838018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run_s001</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.041943</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.700429</td>\n",
       "      <td>0.083779</td>\n",
       "      <td>0.838169</td>\n",
       "      <td>0.838169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run_s002</td>\n",
       "      <td>0.059123</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.699946</td>\n",
       "      <td>0.083957</td>\n",
       "      <td>0.838163</td>\n",
       "      <td>0.838163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run_s003</td>\n",
       "      <td>0.059156</td>\n",
       "      <td>0.041820</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.699243</td>\n",
       "      <td>0.083960</td>\n",
       "      <td>0.838202</td>\n",
       "      <td>0.838202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run_s004</td>\n",
       "      <td>0.058983</td>\n",
       "      <td>0.041593</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.700783</td>\n",
       "      <td>0.083766</td>\n",
       "      <td>0.838191</td>\n",
       "      <td>0.838191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        run  mean_absolute_error  median_absolute_error       mse        r2  \\\n",
       "0  run_s000             0.059128               0.041863  0.007048  0.699714   \n",
       "1  run_s001             0.059143               0.041943  0.007019  0.700429   \n",
       "2  run_s002             0.059123               0.041717  0.007049  0.699946   \n",
       "3  run_s003             0.059156               0.041820  0.007049  0.699243   \n",
       "4  run_s004             0.058983               0.041593  0.007017  0.700783   \n",
       "\n",
       "       rmse  y_avg_pred  y_avg_true  \n",
       "0  0.083951    0.838018    0.838018  \n",
       "1  0.083779    0.838169    0.838169  \n",
       "2  0.083957    0.838163    0.838163  \n",
       "3  0.083960    0.838202    0.838202  \n",
       "4  0.083766    0.838191    0.838191  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
